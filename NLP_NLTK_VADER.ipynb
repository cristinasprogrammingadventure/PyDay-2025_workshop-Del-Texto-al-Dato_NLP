{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Del texto al dato: cómo entienden las máquinas el lenguaje humano con Python**\n",
        "\n",
        "**Fichero de experimentación y mejor comprensión del PLN a través de NLTK - Vader**\n",
        "\n",
        "Este fichero Jupyter Notebook, es una sub-parte del presente taller de **Procesamiento del lenguage natural (NLP)** a través del **análisis de sentimiento y texto**, para experimentación y **exploración más profunda** con el algorítmo Vader de la biblioteca de Python NLTK a fin de **entender de dónde vienen los números** que llevan al algorítmo a calcular, darnos unos resultados u otros."
      ],
      "metadata": {
        "id": "39eXg-8NlCCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerías básicas (manejo de datos y entorno)"
      ],
      "metadata": {
        "id": "11i6jo1Tm6FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== LIBRERÍAS BÁSICAS ====\n",
        "\n",
        "import pandas as pd        # manejo de datos en tablas (DataFrames)\n",
        "import numpy as np         # cálculos numéricos y arrays\n",
        "import re                  # expresiones regulares (limpieza de texto)\n",
        "import string              # manejo de puntuación\n"
      ],
      "metadata": {
        "id": "l67M99qzldPs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerías para procesamiento de texto (NLP / análisis de sentimiento)"
      ],
      "metadata": {
        "id": "UKXcIuLpnHL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== LIBRERÍAS DE PROCESAMIENTO DE TEXTO ====\n",
        "\n",
        "\n",
        "# --- TextBlob: análisis de sentimiento y corrección gramatical ---\n",
        "# from textblob import TextBlob\n",
        "\n",
        "# --- NLTK: herramientas clásicas de NLP ---\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_es = stopwords.words(\"spanish\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-FUJonLfld4y",
        "outputId": "9f80640b-8216-4983-ab2e-2e013550dcae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- spaCy: análisis lingüístico más avanzado (POS, dependencias) ---\n",
        "# !pip install -q spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "# import spacy\n",
        "\n",
        "# --- NRC Emotion Lexicon (opcional, para emociones) ---\n",
        "# !pip install nrclex\n",
        "# from nrclex import NRCLex\n"
      ],
      "metadata": {
        "id": "M9T4IcfC8aXc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the package via spacy.load('en_core_web_sm')\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# prueba\n",
        "# doc = nlp(\"This is a simple test.\")\n",
        "# print([(token.text, token.pos_) for token in doc])\n"
      ],
      "metadata": {
        "id": "_oX-FKwjxRE0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerías para visualización"
      ],
      "metadata": {
        "id": "oW6-cBIgoOQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== LIBRERÍAS DE VISUALIZACIÓN ====\n",
        "\n",
        "import matplotlib.pyplot as plt      # gráficos base\n",
        "import seaborn as sns                # gráficos de alto nivel (opcional)\n",
        "from wordcloud import WordCloud      # nubes de palabras\n",
        "\n",
        "# --- diagramas específicos ---\n",
        "# !pip install matplotlib-venn\n",
        "# from matplotlib_venn import venn2   # comparar vocabulario entre textos\n",
        "\n",
        "# --- estilo gráfico ---\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"Set2\")\n"
      ],
      "metadata": {
        "id": "NgtjVJ7RoOx-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SIMULACRO de pipeline paso a paso con NLTK - Vader !!!**\n",
        "\n",
        "Se han creado unas celdas didácticas que muestran\n",
        "\n",
        "**paso a paso cómo VADER procesa** una frase (**tokeniza, busca en el léxico, aplica reglas** — negación, intensificadores, mayúsculas, signos de exclamación—, suma y normaliza con la fórmula compound).\n",
        "\n",
        "La función está pensada para usarla en un Colab Notebook con ***nltk.sentiment.SentimentIntensityAnalyzer***\n",
        "\n",
        "**Explicación de lo que hace:**\n",
        "\n",
        "- Tokeniza la frase con un método simple (palabras y signos).\n",
        "\n",
        "- Busca cada token en analyzer.lexicon (el diccionario VADER real). Si está, toma su score base; si no, base=0.\n",
        "\n",
        "- Aplica reglas didácticas como lo hace Vader:\n",
        "\n",
        "  - Intensificadores (boosters) en ventana previa multiplican la palabra.\n",
        "\n",
        "  - Negaciones en ventana previa invierten/atenuan.\n",
        "\n",
        "  - ALL CAPS (mayúsculas predominantes) aumentan intensidad.\n",
        "\n",
        "  - Exclamaciones en el texto aumentan ligeramente intensidad.\n",
        "\n",
        "  - Contraste con “but / pero”: multiplica los elementos posteriores (peso más alto para lo posterior).\n",
        "\n",
        "  - Calcula el adjusted_score por token = base × modifier.\n",
        "\n",
        "  - Suma los adjusted_score y aplica la fórmula del compound (la que ya conoces) para normalizar en −1..+1.\n",
        "\n",
        "  - Muestra también el **compound** que devuelve **analyzer.polarity_scores(text)** para comparar textos dferentes.\n"
      ],
      "metadata": {
        "id": "QD3Hjiti2sNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Código — análisis paso a paso (VADER-like, pero algo más sencillo)**\n",
        "\n",
        "- El código es simplificado: **replica las reglas más relevantes** de VADER para que se vea paso a paso lo que ocurre.\n",
        "\n",
        "- VADER real tiene ajustes y detalles más finos (p. ej. constantes ligeramente distintas, tratamiento más preciso de mayúsculas, puntuación, emoticonos y contracciones).\n",
        "\n",
        "- En español la mayor parte de tokens no aparecen en el lexicon\n",
        "  - found = False para muchas palabras (VADER no es recomendable para español sin traducir)."
      ],
      "metadata": {
        "id": "poOsKdWDtmHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecuta antes si aún no lo hiciste\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "import string\n",
        "import math\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Reglas/recursos didácticos (pequeño subconjunto de intensificadores y negaciones)\n",
        "BOOSTER_DICT = {\n",
        "    \"very\": 1.5, \"extremely\": 2.0, \"absolutely\": 1.8, \"really\": 1.5,\n",
        "    \"quite\": 1.2, \"slightly\": 0.5, \"barely\": 0.5\n",
        "}\n",
        "NEGATE = {\"no\", \"not\", \"never\", \"n't\", \"none\", \"nunca\", \"jamás\", \"sin\"}   # incluimos algunos en español para demo\n",
        "CONTRASTIVE = {\"but\", \"pero\", \"aunque\", \"sin embargo\", \"however\", \"nevertheless\"}\n",
        "\n",
        "# Estructura de salida por token\n",
        "TokenInfo = namedtuple(\"TokenInfo\", [\"token\", \"found\", \"base_score\", \"modifier\", \"adjusted_score\", \"notes\"])\n",
        "\n",
        "def _simple_tokenize(text):\n",
        "    # tokenizador simple: separa por espacios y signos, mantiene palabras\n",
        "    text = text.strip()\n",
        "    # conservar ¡¿? etc? simplificamos\n",
        "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
        "    return tokens\n",
        "\n",
        "def compute_compound_from_list(scores):\n",
        "    \"\"\"Aplica la fórmula compound = sum(x) / sqrt(sum(x^2)+15)\"\"\"\n",
        "    sum_x = sum(scores)\n",
        "    sum_x2 = sum([x*x for x in scores])\n",
        "    compound = sum_x / math.sqrt(sum_x2 + 15.0)\n",
        "    # limitar por seguridad\n",
        "    if compound > 1: compound = 1.0\n",
        "    if compound < -1: compound = -1.0\n",
        "    return compound\n",
        "\n",
        "def vader_stepwise(text, analyzer=analyzer, show_details=True):\n",
        "    \"\"\"\n",
        "    Analiza la frase paso a paso con lógica estilo VADER (didáctica, simplificada).\n",
        "    Devuelve: list(TokenInfo), suma, compound_manual, compound_analyzer\n",
        "    \"\"\"\n",
        "    tokens = _simple_tokenize(text)\n",
        "    # lower tokens para lookup, pero conservamos info original\n",
        "    lowered = [t.lower() for t in tokens]\n",
        "\n",
        "    # detectar si hay mayúsculas que sugieran énfasis (más de la mitad de letras del token en mayúscula)\n",
        "    def caps_emphasis(tok):\n",
        "        letters = [c for c in tok if c.isalpha()]\n",
        "        if not letters:\n",
        "            return False\n",
        "        upper_count = sum(1 for c in letters if c.isupper())\n",
        "        return upper_count > 0 and (upper_count / len(letters)) > 0.5\n",
        "\n",
        "    # Recorremos tokens y construimos info\n",
        "    infos = []\n",
        "    for i, tok in enumerate(tokens):\n",
        "        low = lowered[i]\n",
        "        base = None\n",
        "        found = False\n",
        "        notes = []\n",
        "        # buscar en lexicon (vader lexicon está en analyzer.lexicon)\n",
        "        if low in analyzer.lexicon:\n",
        "            base = analyzer.lexicon[low]\n",
        "            found = True\n",
        "            notes.append(\"lexicon\")\n",
        "        else:\n",
        "            # probar lematizar/strip de sufijos simples (didáctico)\n",
        "            # ejemplo: \"delicious\" ok, \"deliciosa\" no → no lo convertimos automáticamente aquí\n",
        "            base = 0.0\n",
        "        # start modifier = 1.0 (sin cambio)\n",
        "        modifier = 1.0\n",
        "\n",
        "        # chequeo de intensificadores en ventana previa (2 tokens previos)\n",
        "        window_start = max(0, i-3)\n",
        "        for j in range(window_start, i):\n",
        "            w = lowered[j]\n",
        "            if w in BOOSTER_DICT:\n",
        "                modifier *= BOOSTER_DICT[w]\n",
        "                notes.append(f\"booster({w})\")\n",
        "\n",
        "        # chequeo de negación en ventana previa (3 tokens)\n",
        "        neg_found = any(w in NEGATE for w in lowered[max(0,i-3):i])\n",
        "        if neg_found:\n",
        "            modifier *= -0.74   # VADER aplica un factor de inversión aproximado (didáctico)\n",
        "            notes.append(\"negation\")\n",
        "\n",
        "        # mayúsculas (énfasis)\n",
        "        if caps_emphasis(tokens[i]):\n",
        "            modifier *= 1.5\n",
        "            notes.append(\"ALLCAPS\")\n",
        "\n",
        "        # puntuación de énfasis: exclamaciones en el texto aumentan (global)\n",
        "        exclamations = text.count(\"!\")\n",
        "        if exclamations > 0:\n",
        "            # factor ligero por exclamaciones (didáctico)\n",
        "            modifier *= (1 + 0.05 * min(exclamations, 4))\n",
        "            notes.append(f\"exclaim({exclamations})\")\n",
        "\n",
        "        # calcular adjusted score: base * modifier\n",
        "        adj = base * modifier if found else 0.0\n",
        "\n",
        "        infos.append(TokenInfo(token=tok, found=found, base_score=base, modifier=modifier, adjusted_score=adj, notes=\", \".join(notes)))\n",
        "\n",
        "    # regla contrastiva (pero / but): dar mayor peso a lo posterior a \"but\"/\"pero\"\n",
        "    # estrategia didáctica: si se encuentra 'but' o 'pero', multiplica por 1.5 los scores posteriores\n",
        "\n",
        "    for i, info in enumerate(infos):\n",
        "        if info.token.lower() in CONTRASTIVE:\n",
        "            # aplicar factor a los tokens posteriores hasta final\n",
        "            for k in range(i+1, len(infos)):\n",
        "                if infos[k].found:\n",
        "                    new_adj = infos[k].adjusted_score * 1.5\n",
        "                    infos[k] = infos[k]._replace(adjusted_score=new_adj, notes=(infos[k].notes + \", contrastive\" if infos[k].notes else \"contrastive\"))\n",
        "\n",
        "    # ahora sumamos los adjusted_score y aplicamos fórmula compound\n",
        "    adjusted_scores = [inf.adjusted_score for inf in infos]\n",
        "    # obtener manual sum only of non-zero\n",
        "    sum_adjusted = sum(adjusted_scores)\n",
        "    compound_manual = compute_compound_from_list(adjusted_scores)\n",
        "\n",
        "    # obtener VADER analyzer compound (para comparar)\n",
        "    try:\n",
        "        vader_scores = analyzer.polarity_scores(text)\n",
        "        compound_analyzer = vader_scores.get(\"compound\", None)\n",
        "    except Exception:\n",
        "        compound_analyzer = None\n",
        "\n",
        "    if show_details:\n",
        "        # mostrar tabla\n",
        "        print(f\"Texto: {text}\\n\")\n",
        "        print(f\"{'token':15s} {'found':7s} {'base':8s} {'modifier':9s} {'adj':8s}  notes\")\n",
        "        print(\"-\"*80)\n",
        "        for inf in infos:\n",
        "            base_s = f\"{inf.base_score:.3f}\" if inf.found else \"---\"\n",
        "            mod_s = f\"{inf.modifier:.3f}\"\n",
        "            adj_s = f\"{inf.adjusted_score:.3f}\"\n",
        "            print(f\"{inf.token:15s} {str(inf.found):7s} {base_s:8s} {mod_s:9s} {adj_s:8s}  {inf.notes}\")\n",
        "        print(\"-\"*80)\n",
        "        print(f\"Sum adjusted scores: {sum_adjusted:.3f}\")\n",
        "        print(f\"Compound (manual formula): {compound_manual:.4f}\")\n",
        "        print(f\"Compound (analyzer.polarity_scores): {compound_analyzer}\\n\")\n",
        "\n",
        "                # === MOSTRAR NEG / NEU / POS justo debajo ===\n",
        "        if vader_scores:\n",
        "            print(\"VADER breakdown\")\n",
        "            print(f\"  NEG: {vader_scores['neg']:.3f}\")\n",
        "            print(f\"  NEU: {vader_scores['neu']:.3f}\")\n",
        "            print(f\"  POS: {vader_scores['pos']:.3f}\")\n",
        "            print(\"-\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "    return infos, sum_adjusted, compound_manual, compound_analyzer\n",
        "\n",
        "# === Ejemplos con los textos en español y traducidos al inglés ===\n",
        "\n",
        "texto1 = \"Volveré ! El servicio es excelente y la comida, deliciosa. El ambiente es agradable. El personal muy atento. Excelente ubicación. Personalmente, me agrada mucho. Sin duda volveré pronto.\"\n",
        "texto2 = \"La espera fue larga y la comida llegó fría. El servicio fue descuidado y la experiencia en general bastante decepcionante.\"\n",
        "texto3 = \"I will return! The service is excellent and the food is delicious. The atmosphere is pleasant. The staff is very attentive. Excellent location. Personally, I like it a lot. I will definitely be back soon.\"\n",
        "texto4 = \"OMG, OH MY GOD !!! The wait was TOO LONG and the food arrived cold. The service was careless and the overall experience was quite disappointing.\"\n",
        "texto5 = \"OMG, OH MY GOD !!! The wait was TOO LONG and the food arrived cold. The service was careless and the overall experience was quite disappointing, but my kids love it.\"  # + parte contrastante \"but my kids love it\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3kMzf_Do2rId",
        "outputId": "90f1d147-4f85-49d0-d13d-d44109f43003"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Obtener POS, NEG, NEU y compound para cada texto ===\n",
        "\n",
        "textos = {\"texto1\": texto1, \"texto2\": texto2, \"texto3\": texto3, \"texto4\": texto4, \"texto5\": texto5}\n",
        "print(\"\\n=== Resultados VADER: NEG / NEU / POS / COMPOUND ===\\n\")\n",
        "\n",
        "for nombre, txt in textos.items():\n",
        "    scores = analyzer.polarity_scores(txt)\n",
        "\n",
        "    # le decimos de imprimir el nombre del texto y (si necesario) el contenido texto mismo {txt}\n",
        "    print(f\"{nombre}: \\n{txt}\")\n",
        "    print(f\"  NEG: {scores['neg']:.3f}\")\n",
        "    print(f\"  NEU: {scores['neu']:.3f}\")\n",
        "    print(f\"  POS: {scores['pos']:.3f}\")\n",
        "    print(f\"  COMPOUND: {scores['compound']:.4f}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V99XdzTqn8_m",
        "outputId": "c0d1712f-f445-4c27-e1b2-9ddcd2850de6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Resultados VADER: NEG / NEU / POS / COMPOUND ===\n",
            "\n",
            "texto1: \n",
            "Volveré ! El servicio es excelente y la comida, deliciosa. El ambiente es agradable. El personal muy atento. Excelente ubicación. Personalmente, me agrada mucho. Sin duda volveré pronto.\n",
            "  NEG: 0.135\n",
            "  NEU: 0.865\n",
            "  POS: 0.000\n",
            "  COMPOUND: -0.5983\n",
            "--------------------------------------------------\n",
            "texto2: \n",
            "La espera fue larga y la comida llegó fría. El servicio fue descuidado y la experiencia en general bastante decepcionante.\n",
            "  NEG: 0.000\n",
            "  NEU: 1.000\n",
            "  POS: 0.000\n",
            "  COMPOUND: 0.0000\n",
            "--------------------------------------------------\n",
            "texto3: \n",
            "I will return! The service is excellent and the food is delicious. The atmosphere is pleasant. The staff is very attentive. Excellent location. Personally, I like it a lot. I will definitely be back soon.\n",
            "  NEG: 0.000\n",
            "  NEU: 0.553\n",
            "  POS: 0.447\n",
            "  COMPOUND: 0.9646\n",
            "--------------------------------------------------\n",
            "texto4: \n",
            "OMG, OH MY GOD !!! The wait was TOO LONG and the food arrived cold. The service was careless and the overall experience was quite disappointing.\n",
            "  NEG: 0.210\n",
            "  NEU: 0.703\n",
            "  POS: 0.087\n",
            "  COMPOUND: -0.6169\n",
            "--------------------------------------------------\n",
            "texto5: \n",
            "OMG, OH MY GOD !!! The wait was TOO LONG and the food arrived cold. The service was careless and the overall experience was quite disappointing, but my kids love it.\n",
            "  NEG: 0.101\n",
            "  NEU: 0.682\n",
            "  POS: 0.217\n",
            "  COMPOUND: 0.7647\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Experimento con textos en español**"
      ],
      "metadata": {
        "id": "lMvoAZ6IO13Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar resultados del pipeline\n",
        "vader_stepwise(texto1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KNAD8_Nc4wCO",
        "outputId": "5209ad41-21e6-4fd1-c27d-4abc00b09f7c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: Volveré ! El servicio es excelente y la comida, deliciosa. El ambiente es agradable. El personal muy atento. Excelente ubicación. Personalmente, me agrada mucho. Sin duda volveré pronto.\n",
            "\n",
            "token           found   base     modifier  adj       notes\n",
            "--------------------------------------------------------------------------------\n",
            "Volveré         False   ---      1.050     0.000     exclaim(1)\n",
            "!               False   ---      1.050     0.000     exclaim(1)\n",
            "El              False   ---      1.050     0.000     exclaim(1)\n",
            "servicio        False   ---      1.050     0.000     exclaim(1)\n",
            "es              False   ---      1.050     0.000     exclaim(1)\n",
            "excelente       False   ---      1.050     0.000     exclaim(1)\n",
            "y               False   ---      1.050     0.000     exclaim(1)\n",
            "la              False   ---      1.050     0.000     exclaim(1)\n",
            "comida          False   ---      1.050     0.000     exclaim(1)\n",
            ",               False   ---      1.050     0.000     exclaim(1)\n",
            "deliciosa       False   ---      1.050     0.000     exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "El              False   ---      1.050     0.000     exclaim(1)\n",
            "ambiente        False   ---      1.050     0.000     exclaim(1)\n",
            "es              False   ---      1.050     0.000     exclaim(1)\n",
            "agradable       False   ---      1.050     0.000     exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "El              False   ---      1.050     0.000     exclaim(1)\n",
            "personal        False   ---      1.050     0.000     exclaim(1)\n",
            "muy             False   ---      1.050     0.000     exclaim(1)\n",
            "atento          False   ---      1.050     0.000     exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "Excelente       False   ---      1.050     0.000     exclaim(1)\n",
            "ubicación       False   ---      1.050     0.000     exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "Personalmente   False   ---      1.050     0.000     exclaim(1)\n",
            ",               False   ---      1.050     0.000     exclaim(1)\n",
            "me              False   ---      1.050     0.000     exclaim(1)\n",
            "agrada          False   ---      1.050     0.000     exclaim(1)\n",
            "mucho           False   ---      1.050     0.000     exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "Sin             True    -2.600   1.050     -2.730    lexicon, exclaim(1)\n",
            "duda            False   ---      -0.777    0.000     negation, exclaim(1)\n",
            "volveré         False   ---      -0.777    0.000     negation, exclaim(1)\n",
            "pronto          False   ---      -0.777    0.000     negation, exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "--------------------------------------------------------------------------------\n",
            "Sum adjusted scores: -2.730\n",
            "Compound (manual formula): -0.5761\n",
            "Compound (analyzer.polarity_scores): -0.5983\n",
            "\n",
            "VADER breakdown\n",
            "  NEG: 0.135\n",
            "  NEU: 0.865\n",
            "  POS: 0.000\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([TokenInfo(token='Volveré', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='!', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='El', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='servicio', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='es', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='excelente', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='y', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='la', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='comida', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token=',', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='deliciosa', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='El', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='ambiente', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='es', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='agradable', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='El', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='personal', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='muy', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='atento', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='Excelente', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='ubicación', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='Personalmente', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token=',', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='me', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='agrada', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='mucho', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='Sin', found=True, base_score=-2.6, modifier=1.05, adjusted_score=-2.7300000000000004, notes='lexicon, exclaim(1)'),\n",
              "  TokenInfo(token='duda', found=False, base_score=0.0, modifier=-0.777, adjusted_score=0.0, notes='negation, exclaim(1)'),\n",
              "  TokenInfo(token='volveré', found=False, base_score=0.0, modifier=-0.777, adjusted_score=0.0, notes='negation, exclaim(1)'),\n",
              "  TokenInfo(token='pronto', found=False, base_score=0.0, modifier=-0.777, adjusted_score=0.0, notes='negation, exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)')],\n",
              " -2.7300000000000004,\n",
              " -0.5761378743726193,\n",
              " -0.5983)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar resultados del pipeline\n",
        "vader_stepwise(texto2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GKHXqb2C7N9x",
        "outputId": "386691d5-4539-4050-a3cb-07b0d0b15c88"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: La espera fue larga y la comida llegó fría. El servicio fue descuidado y la experiencia en general bastante decepcionante.\n",
            "\n",
            "token           found   base     modifier  adj       notes\n",
            "--------------------------------------------------------------------------------\n",
            "La              False   ---      1.000     0.000     \n",
            "espera          False   ---      1.000     0.000     \n",
            "fue             False   ---      1.000     0.000     \n",
            "larga           False   ---      1.000     0.000     \n",
            "y               False   ---      1.000     0.000     \n",
            "la              False   ---      1.000     0.000     \n",
            "comida          False   ---      1.000     0.000     \n",
            "llegó           False   ---      1.000     0.000     \n",
            "fría            False   ---      1.000     0.000     \n",
            ".               False   ---      1.000     0.000     \n",
            "El              False   ---      1.000     0.000     \n",
            "servicio        False   ---      1.000     0.000     \n",
            "fue             False   ---      1.000     0.000     \n",
            "descuidado      False   ---      1.000     0.000     \n",
            "y               False   ---      1.000     0.000     \n",
            "la              False   ---      1.000     0.000     \n",
            "experiencia     False   ---      1.000     0.000     \n",
            "en              False   ---      1.000     0.000     \n",
            "general         False   ---      1.000     0.000     \n",
            "bastante        False   ---      1.000     0.000     \n",
            "decepcionante   False   ---      1.000     0.000     \n",
            ".               False   ---      1.000     0.000     \n",
            "--------------------------------------------------------------------------------\n",
            "Sum adjusted scores: 0.000\n",
            "Compound (manual formula): 0.0000\n",
            "Compound (analyzer.polarity_scores): 0.0\n",
            "\n",
            "VADER breakdown\n",
            "  NEG: 0.000\n",
            "  NEU: 1.000\n",
            "  POS: 0.000\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([TokenInfo(token='La', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='espera', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='fue', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='larga', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='y', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='la', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='comida', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='llegó', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='fría', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='El', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='servicio', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='fue', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='descuidado', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='y', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='la', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='experiencia', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='en', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='general', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='bastante', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='decepcionante', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes=''),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.0, adjusted_score=0.0, notes='')],\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Experimento con textos en inglés**"
      ],
      "metadata": {
        "id": "byeI07w3PCEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar resultados del pipeline\n",
        "vader_stepwise(texto3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-ImzNJ_x7cbf",
        "outputId": "8047f9e4-0e67-4122-df52-be38e2695df9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: I will return! The service is excellent and the food is delicious. The atmosphere is pleasant. The staff is very attentive. Excellent location. Personally, I like it a lot. I will definitely be back soon.\n",
            "\n",
            "token           found   base     modifier  adj       notes\n",
            "--------------------------------------------------------------------------------\n",
            "I               False   ---      1.575     0.000     ALLCAPS, exclaim(1)\n",
            "will            False   ---      1.050     0.000     exclaim(1)\n",
            "return          False   ---      1.050     0.000     exclaim(1)\n",
            "!               False   ---      1.050     0.000     exclaim(1)\n",
            "The             False   ---      1.050     0.000     exclaim(1)\n",
            "service         False   ---      1.050     0.000     exclaim(1)\n",
            "is              False   ---      1.050     0.000     exclaim(1)\n",
            "excellent       True    2.700    1.050     2.835     lexicon, exclaim(1)\n",
            "and             False   ---      1.050     0.000     exclaim(1)\n",
            "the             False   ---      1.050     0.000     exclaim(1)\n",
            "food            False   ---      1.050     0.000     exclaim(1)\n",
            "is              False   ---      1.050     0.000     exclaim(1)\n",
            "delicious       True    2.700    1.050     2.835     lexicon, exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "The             False   ---      1.050     0.000     exclaim(1)\n",
            "atmosphere      False   ---      1.050     0.000     exclaim(1)\n",
            "is              False   ---      1.050     0.000     exclaim(1)\n",
            "pleasant        True    2.300    1.050     2.415     lexicon, exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "The             False   ---      1.050     0.000     exclaim(1)\n",
            "staff           False   ---      1.050     0.000     exclaim(1)\n",
            "is              False   ---      1.050     0.000     exclaim(1)\n",
            "very            False   ---      1.050     0.000     exclaim(1)\n",
            "attentive       False   ---      1.575     0.000     booster(very), exclaim(1)\n",
            ".               False   ---      1.575     0.000     booster(very), exclaim(1)\n",
            "Excellent       True    2.700    1.575     4.253     lexicon, booster(very), exclaim(1)\n",
            "location        False   ---      1.050     0.000     exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "Personally      False   ---      1.050     0.000     exclaim(1)\n",
            ",               False   ---      1.050     0.000     exclaim(1)\n",
            "I               False   ---      1.575     0.000     ALLCAPS, exclaim(1)\n",
            "like            True    1.500    1.050     1.575     lexicon, exclaim(1)\n",
            "it              False   ---      1.050     0.000     exclaim(1)\n",
            "a               False   ---      1.050     0.000     exclaim(1)\n",
            "lot             False   ---      1.050     0.000     exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "I               False   ---      1.575     0.000     ALLCAPS, exclaim(1)\n",
            "will            False   ---      1.050     0.000     exclaim(1)\n",
            "definitely      True    1.700    1.050     1.785     lexicon, exclaim(1)\n",
            "be              False   ---      1.050     0.000     exclaim(1)\n",
            "back            False   ---      1.050     0.000     exclaim(1)\n",
            "soon            False   ---      1.050     0.000     exclaim(1)\n",
            ".               False   ---      1.050     0.000     exclaim(1)\n",
            "--------------------------------------------------------------------------------\n",
            "Sum adjusted scores: 15.698\n",
            "Compound (manual formula): 1.0000\n",
            "Compound (analyzer.polarity_scores): 0.9646\n",
            "\n",
            "VADER breakdown\n",
            "  NEG: 0.000\n",
            "  NEU: 0.553\n",
            "  POS: 0.447\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([TokenInfo(token='I', found=False, base_score=0.0, modifier=1.5750000000000002, adjusted_score=0.0, notes='ALLCAPS, exclaim(1)'),\n",
              "  TokenInfo(token='will', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='return', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='!', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='The', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='service', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='is', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='excellent', found=True, base_score=2.7, modifier=1.05, adjusted_score=2.8350000000000004, notes='lexicon, exclaim(1)'),\n",
              "  TokenInfo(token='and', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='the', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='food', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='is', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='delicious', found=True, base_score=2.7, modifier=1.05, adjusted_score=2.8350000000000004, notes='lexicon, exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='The', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='atmosphere', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='is', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='pleasant', found=True, base_score=2.3, modifier=1.05, adjusted_score=2.415, notes='lexicon, exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='The', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='staff', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='is', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='very', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='attentive', found=False, base_score=0.0, modifier=1.5750000000000002, adjusted_score=0.0, notes='booster(very), exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.5750000000000002, adjusted_score=0.0, notes='booster(very), exclaim(1)'),\n",
              "  TokenInfo(token='Excellent', found=True, base_score=2.7, modifier=1.5750000000000002, adjusted_score=4.2525, notes='lexicon, booster(very), exclaim(1)'),\n",
              "  TokenInfo(token='location', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='Personally', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token=',', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='I', found=False, base_score=0.0, modifier=1.5750000000000002, adjusted_score=0.0, notes='ALLCAPS, exclaim(1)'),\n",
              "  TokenInfo(token='like', found=True, base_score=1.5, modifier=1.05, adjusted_score=1.5750000000000002, notes='lexicon, exclaim(1)'),\n",
              "  TokenInfo(token='it', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='a', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='lot', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='I', found=False, base_score=0.0, modifier=1.5750000000000002, adjusted_score=0.0, notes='ALLCAPS, exclaim(1)'),\n",
              "  TokenInfo(token='will', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='definitely', found=True, base_score=1.7, modifier=1.05, adjusted_score=1.785, notes='lexicon, exclaim(1)'),\n",
              "  TokenInfo(token='be', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='back', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='soon', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.05, adjusted_score=0.0, notes='exclaim(1)')],\n",
              " 15.697500000000002,\n",
              " 1.0,\n",
              " 0.9646)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar resultados del pipeline\n",
        "vader_stepwise(texto4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MwdQWNYnF4xf",
        "outputId": "aec75475-2d53-49ed-8502-2cebccd0077f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: OMG, OH MY GOD !!! The wait was TOO LONG and the food arrived cold. The service was careless and the overall experience was quite disappointing.\n",
            "\n",
            "token           found   base     modifier  adj       notes\n",
            "--------------------------------------------------------------------------------\n",
            "OMG             False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            ",               False   ---      1.150     0.000     exclaim(3)\n",
            "OH              False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            "MY              False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            "GOD             True    1.100    1.725     1.897     lexicon, ALLCAPS, exclaim(3)\n",
            "!               False   ---      1.150     0.000     exclaim(3)\n",
            "!               False   ---      1.150     0.000     exclaim(3)\n",
            "!               False   ---      1.150     0.000     exclaim(3)\n",
            "The             False   ---      1.150     0.000     exclaim(3)\n",
            "wait            False   ---      1.150     0.000     exclaim(3)\n",
            "was             False   ---      1.150     0.000     exclaim(3)\n",
            "TOO             False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            "LONG            False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            "and             False   ---      1.150     0.000     exclaim(3)\n",
            "the             False   ---      1.150     0.000     exclaim(3)\n",
            "food            False   ---      1.150     0.000     exclaim(3)\n",
            "arrived         False   ---      1.150     0.000     exclaim(3)\n",
            "cold            False   ---      1.150     0.000     exclaim(3)\n",
            ".               False   ---      1.150     0.000     exclaim(3)\n",
            "The             False   ---      1.150     0.000     exclaim(3)\n",
            "service         False   ---      1.150     0.000     exclaim(3)\n",
            "was             False   ---      1.150     0.000     exclaim(3)\n",
            "careless        True    -1.500   1.150     -1.725    lexicon, exclaim(3)\n",
            "and             False   ---      1.150     0.000     exclaim(3)\n",
            "the             False   ---      1.150     0.000     exclaim(3)\n",
            "overall         False   ---      1.150     0.000     exclaim(3)\n",
            "experience      False   ---      1.150     0.000     exclaim(3)\n",
            "was             False   ---      1.150     0.000     exclaim(3)\n",
            "quite           False   ---      1.150     0.000     exclaim(3)\n",
            "disappointing   True    -2.200   1.380     -3.036    lexicon, booster(quite), exclaim(3)\n",
            ".               False   ---      1.380     0.000     booster(quite), exclaim(3)\n",
            "--------------------------------------------------------------------------------\n",
            "Sum adjusted scores: -2.864\n",
            "Compound (manual formula): -0.5160\n",
            "Compound (analyzer.polarity_scores): -0.6169\n",
            "\n",
            "VADER breakdown\n",
            "  NEG: 0.210\n",
            "  NEU: 0.703\n",
            "  POS: 0.087\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([TokenInfo(token='OMG', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token=',', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='OH', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='MY', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='GOD', found=True, base_score=1.1, modifier=1.7249999999999999, adjusted_score=1.8975, notes='lexicon, ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='!', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='!', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='!', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='The', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='wait', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='was', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='TOO', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='LONG', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='and', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='the', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='food', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='arrived', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='cold', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='The', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='service', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='was', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='careless', found=True, base_score=-1.5, modifier=1.15, adjusted_score=-1.7249999999999999, notes='lexicon, exclaim(3)'),\n",
              "  TokenInfo(token='and', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='the', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='overall', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='experience', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='was', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='quite', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='disappointing', found=True, base_score=-2.2, modifier=1.38, adjusted_score=-3.036, notes='lexicon, booster(quite), exclaim(3)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.38, adjusted_score=0.0, notes='booster(quite), exclaim(3)')],\n",
              " -2.8635,\n",
              " -0.5160219476166336,\n",
              " -0.6169)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar resultados del pipeline\n",
        "vader_stepwise(texto5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rKRtRiaf8EP_",
        "outputId": "396ac8df-9f50-4b9c-e566-037b5ed34a61"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: OMG, OH MY GOD !!! The wait was TOO LONG and the food arrived cold. The service was careless and the overall experience was quite disappointing, but my kids love it.\n",
            "\n",
            "token           found   base     modifier  adj       notes\n",
            "--------------------------------------------------------------------------------\n",
            "OMG             False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            ",               False   ---      1.150     0.000     exclaim(3)\n",
            "OH              False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            "MY              False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            "GOD             True    1.100    1.725     1.897     lexicon, ALLCAPS, exclaim(3)\n",
            "!               False   ---      1.150     0.000     exclaim(3)\n",
            "!               False   ---      1.150     0.000     exclaim(3)\n",
            "!               False   ---      1.150     0.000     exclaim(3)\n",
            "The             False   ---      1.150     0.000     exclaim(3)\n",
            "wait            False   ---      1.150     0.000     exclaim(3)\n",
            "was             False   ---      1.150     0.000     exclaim(3)\n",
            "TOO             False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            "LONG            False   ---      1.725     0.000     ALLCAPS, exclaim(3)\n",
            "and             False   ---      1.150     0.000     exclaim(3)\n",
            "the             False   ---      1.150     0.000     exclaim(3)\n",
            "food            False   ---      1.150     0.000     exclaim(3)\n",
            "arrived         False   ---      1.150     0.000     exclaim(3)\n",
            "cold            False   ---      1.150     0.000     exclaim(3)\n",
            ".               False   ---      1.150     0.000     exclaim(3)\n",
            "The             False   ---      1.150     0.000     exclaim(3)\n",
            "service         False   ---      1.150     0.000     exclaim(3)\n",
            "was             False   ---      1.150     0.000     exclaim(3)\n",
            "careless        True    -1.500   1.150     -1.725    lexicon, exclaim(3)\n",
            "and             False   ---      1.150     0.000     exclaim(3)\n",
            "the             False   ---      1.150     0.000     exclaim(3)\n",
            "overall         False   ---      1.150     0.000     exclaim(3)\n",
            "experience      False   ---      1.150     0.000     exclaim(3)\n",
            "was             False   ---      1.150     0.000     exclaim(3)\n",
            "quite           False   ---      1.150     0.000     exclaim(3)\n",
            "disappointing   True    -2.200   1.380     -3.036    lexicon, booster(quite), exclaim(3)\n",
            ",               False   ---      1.380     0.000     booster(quite), exclaim(3)\n",
            "but             False   ---      1.380     0.000     booster(quite), exclaim(3)\n",
            "my              False   ---      1.150     0.000     exclaim(3)\n",
            "kids            False   ---      1.150     0.000     exclaim(3)\n",
            "love            True    3.200    1.150     5.520     lexicon, exclaim(3), contrastive\n",
            "it              False   ---      1.150     0.000     exclaim(3)\n",
            ".               False   ---      1.150     0.000     exclaim(3)\n",
            "--------------------------------------------------------------------------------\n",
            "Sum adjusted scores: 2.656\n",
            "Compound (manual formula): 0.3394\n",
            "Compound (analyzer.polarity_scores): 0.7647\n",
            "\n",
            "VADER breakdown\n",
            "  NEG: 0.101\n",
            "  NEU: 0.682\n",
            "  POS: 0.217\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([TokenInfo(token='OMG', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token=',', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='OH', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='MY', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='GOD', found=True, base_score=1.1, modifier=1.7249999999999999, adjusted_score=1.8975, notes='lexicon, ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='!', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='!', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='!', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='The', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='wait', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='was', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='TOO', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='LONG', found=False, base_score=0.0, modifier=1.7249999999999999, adjusted_score=0.0, notes='ALLCAPS, exclaim(3)'),\n",
              "  TokenInfo(token='and', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='the', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='food', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='arrived', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='cold', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='The', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='service', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='was', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='careless', found=True, base_score=-1.5, modifier=1.15, adjusted_score=-1.7249999999999999, notes='lexicon, exclaim(3)'),\n",
              "  TokenInfo(token='and', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='the', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='overall', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='experience', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='was', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='quite', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='disappointing', found=True, base_score=-2.2, modifier=1.38, adjusted_score=-3.036, notes='lexicon, booster(quite), exclaim(3)'),\n",
              "  TokenInfo(token=',', found=False, base_score=0.0, modifier=1.38, adjusted_score=0.0, notes='booster(quite), exclaim(3)'),\n",
              "  TokenInfo(token='but', found=False, base_score=0.0, modifier=1.38, adjusted_score=0.0, notes='booster(quite), exclaim(3)'),\n",
              "  TokenInfo(token='my', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='kids', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='love', found=True, base_score=3.2, modifier=1.15, adjusted_score=5.52, notes='lexicon, exclaim(3), contrastive'),\n",
              "  TokenInfo(token='it', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)'),\n",
              "  TokenInfo(token='.', found=False, base_score=0.0, modifier=1.15, adjusted_score=0.0, notes='exclaim(3)')],\n",
              " 2.6564999999999994,\n",
              " 0.3393968115580649,\n",
              " 0.7647)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Observaciones y explicaciones:** usando el ejemplo del ***texto5***:\n",
        "\n",
        "  ***\"OMG, OH MY GOD !!! The wait was TOO LONG and the food arrived cold. The service was careless and the overall experience was quite disappointing, but my kids love it.\"***\n",
        "\n",
        "- Los signos “**!**” y otros signos y palabras → “**found = False**”\n",
        "  - NO significa que “VADER no los reconozca”\n",
        "  - significa que **no está** en el ***lexicon*** de **palabras con sentimiento**\n",
        "  - =>  **no aportan sentimiento propio** → base = 0 y adjusted = 0\n",
        "  - Sí que **las reconoce** y pueden acaban **afectando el cálculo final** ('interpretación') **si afectan el modifier**\n",
        "\n",
        "#### **QUÉ ES “modifier”**\n",
        "  \n",
        "  - modifier refleja el **estado de intensidad acumulado**, el **factor multiplicador final** que se aplica al *base_score* de cada *token* con **sentimiento**, que luego se aplicaría a la **siguiente palabra con sentimiento**, si existiera\n",
        "\n",
        "### **QUÉ son los Booster words (intensificadores/atenuadores)**\n",
        "\n",
        "  - Ej.: very, extremely, quite, barely…\n",
        "Modifican ±10% o ±15% el valor de la palabra a la derecha.\n",
        "\n",
        "    quite → +0.15 sobre palabras positivas\n",
        "\n",
        "    quite → −0.15 sobre palabras negativas\n",
        "\n",
        "    => cuando aparece “quite” antes de disappointing se aplica: modifier = 1 + 0.15 = 1.15\n",
        "\n",
        "    => si hay signos de exclamación (!), se suma más\n",
        "\n",
        "  - Ejemplo:\n",
        "\n",
        "    **El resultado !     False   --->   1.150 ->  0.000 ->  exclaim(3) Significa:**\n",
        "\n",
        "    - found=False → “!” no es una palabra del léxico emocional, pero contará como amplificador.\n",
        "\n",
        "    - modifier=1.150 → los signos de exclamación están amplificando el sentimiento de palabras cercanas.\n",
        "\n",
        "    - adjusted_score=0.000 → al no tener base_score, ellos solos no suman ni restan.\n",
        "\n",
        "    - exclaim(3) → se detectaron 3 signos “!”, y VADER eleva la excitación general del texto.\n",
        "\n",
        "### **QUÉ son ALL CAPS (*capital letters* - todo mayúsculas)**\n",
        "\n",
        "  - Ej.: OMG, OH, MY, TOO, LONG\n",
        "\n",
        "Veamos la línea:\n",
        "\n",
        "TokenInfo(token='**GOD**', found=True, base_score=1.1, modifier=1.7249, adjusted_score=1.8975, notes='lexicon, ALLCAPS, exclaim(3)'\n",
        "\n",
        "- “OMG, OH, MY” no están en el léxico → no suman nada\n",
        "\n",
        "- Pero sí reciben modifiers (mayúsculas, exclamaciones), aunque multiplican 0 → 0.\n",
        "\n",
        "- El único término que contribuye a la emoción de ese bloque es *GOD*, por eso genera una 'positividad artificial'.\n",
        "\n",
        "\n",
        "  - Ej.: POR QUÉ \"**OMG** / **OH MY GOD**\" dispara valores raros\n",
        "\n",
        "    - “**GOD**” está en el **léxico como positivo** (valor base +1.1)\n",
        "\n",
        "    - Está en mayúsculas → se refuerza modifier => modifier + 0.733 (≈ **+73% de intensidad**)\n",
        "\n",
        "    - Va seguido de “!!!” → se refuerza más\n",
        "    - Los modifiers de *GOD* son 1.725 (≈ 1 + 0.733).\n",
        "\n",
        "\n",
        "    - **¿Cuál es el peso total del token “GOD” en este texto**?\n",
        "\n",
        "      +1.10 → sentimiento base en el léxico\n",
        "\n",
        "      ×1.725 → amplificación por mayúsculas y signos de exclamación\n",
        "\n",
        "      = 1.10 × 1.725 = 1.8975\n",
        "\n",
        "      = 1.90 → puntuación final ajustada (positivo artificial)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Exclamation (!)**\n",
        "\n",
        "En la tabla resultante siempre **aparece 1.15 cuando hay un “nivel bajo” de excitación** aplicando así este valor moderado (≈ +15%) cuando no hay refuerzo especial ni palabras en mayúsculas.\n",
        "\n",
        "VADER aumenta la intensidad según cuántos “!” encuentre (máx. 4):\n",
        "\n",
        "- Luego, cada '**!**' suma → +0.292\n",
        "- si hay 4 -> +0.292 x 4 = 1,168\n",
        "\n",
        "\n",
        "### **Contrast (“but”, “however”)**\n",
        "\n",
        "- Cuando aparece “but”, VADER reduce la intensidad del sentimiento de la parte antes de “but” (*pero*)\n",
        "\n",
        "- aumenta la intensidad de lo que está después\n",
        "\n",
        "- Eso explica por qué ***love recibe “contrastive***”.\n",
        "\n",
        "- Por eso love termina con modifier 1.15 pero su efecto domina en el compound final\n",
        "\n",
        "Veamos la línea :\n",
        "\n",
        "TokenInfo(token='**love**', found=**True**, **base_score=3.2**, **modifier=1.15**, **adjusted_score=5.52**, **notes**='lexicon, exclaim(3), contrastive\n",
        "\n",
        "  -   podemos calcular 3.2 * 1.15 = 3.68 (no 5.520 !) -> Esto solo aplica el modifier local (mayúsculas, exclamaciones, boosters)\n",
        "  - falta un paso más del algoritmo:\n",
        "    - aplicar el “**contrastive shift**” por ***BUT*** (“***Pseudorule of contrastive conjunctions***”)\n",
        "      - VADER **reduce el peso de los sentimientos anteriores** y **aumenta el sentimiento posterior** aproximadamente x1.5, por lo que tendremos:\n",
        "      - **base_score** x **modifier** x **contrastive boost** = **adjusted_score** (peso real de la palabra)\n",
        "\n",
        "        \n",
        "      - Con ello vemos que aumenta el peso del sentimiento que viene después del “but”\n",
        "\n",
        "    - ¿Cuál es el peso total del **token “love”** en **este texto**?\n",
        "\n",
        "        3.20 → sentimiento base de “love” en el léxico\n",
        "\n",
        "        ×1.15 → amplificación por contexto (mayúsculas, exclamación, etc.)\n",
        "\n",
        "        ×1.50 → boost adicional porque viene después de “but” (regla contrastiva)\n",
        "\n",
        "      = 3.2 × 1.15 × 1.5 = 5.52  \n",
        "      \n",
        "      = 5.52 → puntuación final ajustada\n",
        "\n",
        "Su peso es 5.52, dominando el análisis porque aparece después de “but”,\n",
        "y VADER está **diseñado para priorizar “la cláusula final”** en frases **contrastivas** siendo una **regla lingüística preprogramada** del inglés, si tenemos “It was bad, but I loved it” → **el sentimiento final domina**, no siendo solo un matiz.\n",
        "\n",
        "### **RESUMEN DE REGLAS AFECTANDO AL CÁLCULO GENERAL**\n",
        "\n",
        "**modifier** = **combinación de**:\n",
        "\n",
        "- Booster words\n",
        "- ALL CAPS\n",
        "- Exclamation\n",
        "- Reglas de contraste (“but”, \"however\")\n",
        "- Negación (no aparece en este texto, pero podrá ser \"***not***\" como en \"***not nice***\")\n",
        "- Algunas otras reglas y matices\n"
      ],
      "metadata": {
        "id": "q2yH7ScRIptG"
      }
    }
  ]
}